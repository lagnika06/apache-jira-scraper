ğŸ§  Apache JIRA Scraper
ğŸš€ Overview

The Apache JIRA Scraper automates the extraction and transformation of issue data from various Apache Software Foundation projects using the JIRA REST API.

It collects raw issue metadata (ID, project, summary, status, timestamps, etc.), cleans and standardizes the data into structured JSONL format, and prepares it for downstream analytics, dashboards, or machine learning pipelines.

This project demonstrates a complete data engineering mini-pipeline â€” from API ingestion to data transformation and storage â€” with modular, production-ready code.

ğŸ—‚ï¸ Project Structure
apache-jira-scraper/
â”‚
â”œâ”€â”€ scraper/
â”‚   â”œâ”€â”€ config.py            # API configuration and project settings
â”‚   â”œâ”€â”€ jira_scraper.py      # Fetches issues from Apache JIRA REST API
â”‚   â”œâ”€â”€ data_transformer.py  # Cleans, normalizes, and saves structured data
â”‚   â””â”€â”€ utils.py             # Logging, helper, and utility functions
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # Stores raw API responses (ignored in .gitignore)
â”‚   â””â”€â”€ processed/           # Stores cleaned JSONL outputs (ignored in .gitignore)
â”‚
â”œâ”€â”€ logs/                    # Application logs (ignored in .gitignore)
â”‚
â”œâ”€â”€ main.py                  # Entry point: runs the complete pipeline
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ .gitignore               # Ignore rules for large or transient files
â””â”€â”€ README.md                # Documentation

âš™ï¸ Installation
# Clone the repository
git clone https://github.com/lagnika06/apache-jira-scraper.git
cd apache-jira-scraper

# Create and activate virtual environment
python3 -m venv venv
source venv/bin/activate     # On Mac/Linux
venv\Scripts\activate        # On Windows

# Install dependencies
pip install -r requirements.txt

â–¶ï¸ Usage

Run the entire scraping and transformation pipeline:

python main.py


This will:

Fetch JIRA issues from Apache projects (defined in scraper/config.py)

Save the raw responses in data/raw/

Clean and normalize the data

Store structured output as JSONL files in data/processed/

ğŸ“ Example Output

Example record from data/processed/sample.jsonl:

{
  "id": "KAFKA-14873",
  "project": "Kafka",
  "summary": "Fix consumer offset tracking issue",
  "status": "Resolved",
  "created": "2025-10-14T10:45:00Z",
  "updated": "2025-10-17T09:12:00Z",
  "assignee": "JohnDoe"
}

ğŸ’¡ Key Features

âœ… Automated Data Scraping â€“ Extracts issue data via JIRA REST API
âœ… Config-Driven Design â€“ All project and endpoint settings stored in config.py
âœ… Data Transformation â€“ Cleans, normalizes, and outputs structured JSONL
âœ… Logging & Error Handling â€“ Tracks each step with retry logic for failures
âœ… Modular Codebase â€“ Separate layers for scraping, transforming, and utilities
âœ… Version Control Ready â€“ Clean .gitignore, .gitkeep placeholders

ğŸ§© Tech Stack
Category	Tools / Libraries
Language	Python 3.9+
Data Handling	JSON / JSONL
API Client	requests
Logging	logging
Virtual Environment	venv
ğŸ“Š Workflow Overview
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  Apache JIRA API    â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  jira_scraper.py         â”‚  â†’ Fetch raw JSON
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  data_transformer.py     â”‚  â†’ Clean, normalize, convert
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   data/processed/*.jsonl â”‚  â†’ Ready for analytics
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

âš ï¸ Data Notes

ğŸ“Œ The raw and processed data folders are excluded from GitHub to avoid large file uploads (>25 MB).
Empty directories are preserved using .gitkeep files and are automatically populated when the scraper runs.

ğŸš€ Future Enhancements

 Integrate with Elasticsearch or BigQuery for scalable storage

 Add Airflow or cron jobs for scheduled scraping

 Include data visualization dashboards (issue trends, resolution times)

 Implement unit tests for scraper and transformer modules

ğŸ§‘â€ğŸ’» Author

Lagnika Dagur
ğŸ“ B.Tech, Computer Science & Engineering
ğŸ’¼ AI | Cloud | Data Engineering Enthusiast
ğŸŒ GitHub: lagnika06


ğŸŒŸ Acknowledgments

Special thanks to Scaler Academy for the opportunity to implement real-world engineering projects involving data scraping, transformation, and structured pipelines.
